Стратегия автоматизации тестирования
Ссылки:

Left Shift Testing
В этом уроке мы разберём эталонный процесс автоматизации тестирования.

Важно! Описанная концепция — это целевая модель, к которой стоит стремиться. Это не означает, что процесс должен быть реализован строго в таком виде. Возможны некоторые отклонения: каждая компания, каждый продукт и процессы вокруг релизов (от раз в полгода до нескольких раз в день) сильно различаются.

Тем не менее, представленный процесс является наилучшей практикой, выстроенной на основе реального опыта. Когда вы придёте в компанию, скорее всего там уже будет существовать свой процесс. У вас будет возможность его улучшить. Если же процесса нет — это отличная возможность построить всё с нуля и применить лучшие практики.

1. Начало разработки
Когда задача попадает в разработку, тестировать ещё нечего — функционал находится в процессе реализации. Однако уже на этом этапе QA Automation Engineer может начать подготовку:

Создать страницы/элементы для будущих UI-тестов
Подготовить шаблоны тестов (болванки)
Добавить маркировки и теги
Прописать необходимые фикстуры, API-клиенты, pydantic модели
Договориться о добавлении нужных идентификаторов на страницы
Иными словами, мы готовим инфраструктуру для будущих автотестов, чтобы затем не терять время.

2. Работа в фича-ветке
Как только разработчик завершает реализацию задачи, мы подключаемся. Идеальный процесс подразумевает, что тесты пишутся прямо в той же ветке, где реализована фича:

Если это UI-тесты, мы поднимаем локально фронтенд-приложение, мокаем серверную часть и пишем тесты.
Если это API-тесты, поднимаем сервис локально, мокаем внешние зависимости (БД, редис, очереди, внешние интеграции и другие).
Запустить окружение локально сегодня очень просто — в основном всё решается через Docker. Если сервис пока не имеет готовых скриптов запуска, можно попросить разработчиков помочь — для них это несложная задача.

Цель этого этапа — полноценное локальное тестирование фичи без необходимости деплоя на окружение.



Локальное тестирование ("Left Shift Testing")
Процесс, когда тестирование сдвигается влево (до выкладки на окружение), называется Left Shift Testing. Что это даёт:

Быстрая коммуникация с разработчиком. Тестировщик и разработчик работают "плечом к плечу" в одной ветке, без стен между ними.
Мгновенная обратная связь. Нашли баг — сразу сообщаем разработчику. Разработчик сразу исправляет баг и коммитит изменения в эту же ветку. Мы подтягиваем изменения без ожидания деплоев.
Экономия времени на релиз. Один деплой на окружение может занимать 20–30 минут. При классическом процессе (баг – возврат задачи – починка – деплой) можно потратить 4 часа только на ожидания! Здесь же исправление занимает несколько минут.
Отсутствие коллизий между фичами. Мы тестируем фичу в изоляции, до слияния в основную ветку. Если что-то сломалось — мы точно знаем, где.
3. Выгрузка тестов на CI/CD
Когда автотесты написаны и фича локально протестирована:

Мы пушим изменения в репозиторий. Приложение и тесты находятся в одном репозитории.
Наши тесты начинают запускаться на CI/CD вместе с фичей. На CI/CD поднимается сначала сервис/приложение, затем запускаются тесты.
Если сервис и тесты в одном репозитории — можно попросить разработчиков помочь:
Поднять сам сервис в CI/CD-пайплайне
Затем запустить тесты
Это называется изоляционное тестирование (Isolation Testing).

Преимущества изоляционного тестирования
Упрощение коммуникации.
Разработчик и тестировщик работают в одном репозитории и одной ветке.
Нет долгих циклов ожидания деплоя на dev/staging для каждого исправления бага.
Всё фиксится и проверяется мгновенно: коммит → тест → фидбек за минуты, а не часы.
Быстрая локализация багов
Баги находятся здесь и сейчас, пока фича ещё в своей ветке.
Нет путаницы, кто сломал тесты (как бывает, когда в стенд залетает много фич одновременно).
Баги фиксируются до деплоя фичи на dev/stable/staging.
Очень быстрые и стабильные тесты
Используются моковые данные вместо "живых" стендов.
Тесты получаются короткими, атомарными и стабильными.
Минимизируется влияние нестабильности окружения.
Тесты падают только при реальных багах, а не из-за внешних факторов.
Тесты приносят реальную пользу
Тесты — часть проекта, а не отдельная сущность.
Разработчики вовлечены в тесты: знают о них, помогают исправлять, вникают.
Выстраивается культура тесного взаимодействия между тестировщиками и разработчиками.
4. Деплой на окружение и интеграционное тестирование
Только после успешного локального тестирования:

Фича деплоится на окружение (Dev/Stable/Staging)
Запускается скоуп стабильных интеграционных тестов (sanity/smoke)
Цель интеграционных тестов:

Не протестировать саму фичу (она уже протестирована локально!)
А убедиться, что фича не сломала основную систему
При необходимости пишутся новые или исправляются старые интеграционные тесты
Если интеграционные тесты находят баги:

Разработчик вносит фиксы в фича-ветку
Процесс повторяется: изоляционные тесты → интеграционные тесты
Когда все проверки пройдены:

Собирается новый тег/версия
Перед продакшен-деплоем дополнительно гоняется регрессионный скоуп изоляционных тестов прямо в репозитории сервиса/приложения, чтобы убедиться, что раскатанная версия полностью рабочая.
Если тесты проходят, то фича едет в прод.



Пример процесса на схемах
На схеме ниже — полный процесс с изоляционными и интеграционными тестами:



На второй схеме — упрощённый процесс, где сразу после деплоя запускаются только интеграционные тесты:



Такой упрощённый вариант можно использовать:

В начале проекта
При ограничениях по времени
Если команда ещё не готова к полноценной изоляционной автоматизации
Какие риски закрываются данным процессом?
Давайте сломаем описанный процесс и представим, что автоматизация тестирования существует отдельно от разработки. Тесты пишутся не в процессе реализации и тестирования фичи, а уже после её выкатки, с заметным опозданием. Что происходит в таком случае?

Автоматизация теряет смысл. Если фича уехала с багами, мы об этом узнаём слишком поздно. Тесты не запускаются "здесь и сейчас", баги находят постфактум, иногда уже на продакшене. Соответственно, растут риски выката не протестированных релизов.
Разрыв между разработкой и тестированием. Автоматизатор оказывается оторванным от реальных процессов команды. Нет глубокого понимания, как работает система и фичи, есть только поверхностные знания. Это приводит к ошибкам в тестировании и непониманию причин сбоев.
Нет локализации проблем. Когда тесты начинают падать, мы не знаем, кто и где их сломал. Начинаются долгие выяснения: кто виноват — тесты или код? Хотя при изоляционном тестировании мы сразу знаем, где именно и в какой ветке произошла проблема.
Отсутствие вовлечения разработчиков. В изоляционной модели разработчик видит падение тестов сразу в своей фиче-ветке и может сам починить тесты вместе с багфиксом. Наша задача — только проверить и заапрувить. Без тесной интеграции такую быструю реакцию мы теряем.
Проблемы со стабильностью тестов. При отказе от изоляционного подхода тесты гоняются только на реальных окружениях, которые нестабильны по многим причинам: малоресурсные стенды, рассинхрон микросервисов, нестабильные интеграции с внешними партнёрами. Это делает тесты медленными и ненадёжными.
Падение доверия к тестам. Постоянно "красные" тесты демотивируют команду. Тесты начинают игнорировать ("они всегда падают, ничего страшного"), и в итоге они перестают быть индикатором качества, превращаясь в обузу, которую нужно постоянно чинить без реальной пользы.
Реально ли это?
Да, абсолютно реально. И если на первый взгляд процесс кажется сложным или даже невозможным — скорее всего, это говорит не о его нереалистичности, а о нехватке практического опыта в автоматизации тестирования, понимании CI/CD, инфраструктуры или построении зрелых инженерных процессов.

Это не упрёк — это нормальный этап развития. Всё, что описано в статье, не теория: процесс уже успешно работает в десятках команд и проектах, был отточен за год реального боевого использования, и доказал свою надёжность.

Он не родился из воздуха. Он появился как ответ на реальные проблемы и боли. И если хочется внедрить по-настоящему устойчивую практику автотестирования — другого пути просто нет.

Можно идти к этому поэтапно, не обязательно внедрять всё сразу. Главное — не останавливаться на "у нас не получится", а разбираться, пробовать, задавать вопросы и расти как инженер.