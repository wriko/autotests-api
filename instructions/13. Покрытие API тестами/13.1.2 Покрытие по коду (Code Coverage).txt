
Code coverage (покрытие по коду) — это один из самых базовых и широко используемых способов измерения тестового покрытия.
Он показывает, насколько хорошо исходный код покрыт тестами, то есть какие строки кода были выполнены во время выполнения тестов.

------------------------------------
Где применяется покрытие по коду?
------------------------------------
Этот метод в первую очередь используется разработчиками на стороне серверной логики, когда они пишут юнит-тесты. Именно в таких случаях можно точно измерить, какие участки кода исполняются, а какие остаются нетронутыми.

С точки зрения автоматизированных API тестов, покрытие по коду практически не применимо, так как:
 - API тесты взаимодействуют только с публичным интерфейсом (через HTTP/gRPC/WebSocket).
 - У нас нет доступа к исходному коду сервера, следовательно, мы не можем отследить, какой код был выполнен внутри приложения.
Таким образом, покрытие по коду — это инструмент исключительно для разработчиков, а не для QA-инженеров, работающих с интеграционными или e2e тестами.

------------------------------------
Основной минус code coverage — неочевидность
------------------------------------
Даже если у нас есть доступ к покрытию, оно часто мало что говорит с точки зрения бизнес-логики. Например, метрика:

“Класс UsersRepositoryController покрыт на 98%”

— вроде бы звучит хорошо, но что именно протестировано?, какой функционал закрыт? — остаётся неясным.

Code coverage показывает только, какие строчки кода были выполнены, но не показывает, тестируется ли бизнес-сценарий или пользовательский путь.

------------------------------------
Как выглядит покрытие по коду на практике?
------------------------------------
Рассмотрим небольшой пример с использованием библиотеки coverage — это одна из самых популярных утилит в Python для измерения покрытия.

Шаг 1. Установка библиотеки
Установим coverage:
    pip install coverage
coverage.py — это инструмент, который отслеживает, какие строки Python-кода выполняются во время тестов. Он сохраняет эту информацию и может представить её в виде отчёта.

Шаг 2. Запуск тестов с измерением покрытия
Выполним команду из корня проекта autotests-api:
    coverage run -m pytest -m "regression"
После выполнения в корне проекта появится файл .coverage, содержащий данные о покрытии.

Шаг 3. Генерация HTML отчёта
Для просмотра отчета в браузере сгенерируем его в формате HTML:
    coverage html
В результате будет создана папка htmlcov, в которой появится файл index.html. Открыв его в браузере, мы увидим визуальный отчёт:
 - список всех файлов проекта,
 - уровень покрытия по каждому из них,
 - цветовую подсветку строк (покрытые, частично покрытые, не покрытые).
Пример отчёта:



Далее мы можем кликнуть на интересующий нас файл, чтобы открыть его и посмотреть более детальную информацию о покрытии — какие строки были выполнены во время тестов, а какие остались нетронутыми:



Важно! В нашем случае, если мы применим coverage к API автотестам, мы на самом деле измерим покрытие самих тестов, а не тестируемого приложения. То есть мы увидим:

какие модули в папке tests/ или clients/ были выполнены,
какие строки в наших тестах запустились.
Это не то же самое, что проверка логики сервера. Для оценки покрытия именно тестируемого API нужен другой подход — например, измерение покрытия Swagger-схемы, о котором мы поговорим в следующем уроке.


Плюсы и минусы покрытия по коду

Плюсы:
 - Минимальные усилия по настройке. Всё просто: установили библиотеку, запустили тесты — и сразу получили HTML-отчёт. Никакой сложной конфигурации не требуется.
 - Позволяет измерить покрытие серверного кода. Если на сервер написаны unit-тесты, можно быстро понять, насколько хорошо они охватывают бизнес-логику и критические участки кода. Это полезно для backend-команд, особенно при регрессионном тестировании.

Минусы:
 - Недостаток бизнес-информативности. Отчёт по коду не отвечает на вопросы про функциональность, с которой работают пользователи. Например, если проджект-менеджер спросит: «А покрыт ли у нас автотестами функционал поиска по банковским операциям?» — глядя на кодовый отчёт мы не сможем ответить точно. Код, связанный с поиском, может быть разбросан по десяткам файлов, и единственное, что мы можем сказать: «У нас общее покрытие 95%, значит, наверное, и поиск покрыт…». Но это предположение, не факт. Отчёт не отображает покрытие бизнес-требований, только технических строк кода.
 - Непригодность для API-тестов. Покрытие по коду эффективно только для unit-тестов, которые исполняются внутри кода. А в случае с API-тестами мы взаимодействуем снаружи — через HTTP/gRPC/WebSocket. Мы не имеем доступа к серверному коду, не можем поставить туда "датчики" покрытия. Максимум, что мы можем — посмотреть, насколько покрыт сам тестовый код, но не то приложение, которое мы тестируем.
 - Отчёт малоинформативен для других ролей. Такие отчёты понятны в основном разработчикам или, в лучшем случае, опытным QA Automation-инженерам. Для ручных тестировщиков, бизнес-аналитиков или проджект-менеджеров этот отчёт будет либо непонятен, либо просто бесполезен. А хотелось бы иметь такой отчёт, который можно показать любой роли в команде — и он даст чёткое понимание: что именно протестировано, а что нет.


Вывод
Измерение покрытия по коду — полезный инструмент, но не для нас как для QA Automation Engineers, пишущих API-тесты. Это больше про внутреннюю кухню backend-команд. Такой отчёт стоит иметь на проекте как дополнительную метрику, но он не даёт полной картины покрытия с точки зрения требований и пользовательского опыта.